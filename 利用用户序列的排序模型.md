# 背景和主题
在众多高中频应用领域，用户历史行为构成了用户侧主要的信息。
如何充分得利用这些信息对推荐算法和在线推理构成了巨大的挑战。

这个文档主要是介绍和总结个人实践过的利用用户行为序列的排序模型，包括
- 传统slots+embedding方法
- slot+embedding+pooling方法
- din 
- bst
- 改进bst(在雷鸟的实践)

## 传统方法
工业界普遍的排序系统都是slots+embedding方法，也就是多个特征域，
每个域都是离散特征，然后分别经过embedding层后拼接最后多层mlp输出单个或多个得分(比如ctr和cvr分)。  
用户行为序列也不例外,一般是固定行为长度，不足则补齐。  
这样的方法在线推理方便，不需要改动结构框架。

## 传统方法+pooling
传统方法对应于行为序列的mlp层参数比较多，固定行为长度中每个位置的参数都在统计上面临训练不足的问题，
且历史交互物品顺序上的改变就会影响mlp层的输入。假设一个用户在最近交互的一个物品前后，兴趣很可能并没有变化很大，但mlp层对应行为序列的slots那里就产生了一个偏移，输入完全变了。
所以在embedding层后加入max pooling或者sum pooling，可以更简练得总结用户历史兴趣。

## din
传统方法+pooling的方法简介得表达了用户的兴趣，但是和目标物品独立。比如为用户推荐饭店和鞋子时，
用户兴趣没有分别强调历史上对同品类的偏好。din出于这点考虑，用目标物品和历史物品的相似度分数来作为sum的权重。


## bst
17年发表的transformer在nlp届大杀四方，它的核心组件--self attention,就是一个序列输入，每个step生成一组q, k, v向量，借鉴information retrieval领域的术语(query, key, value)，用自己的q乘全部steps的k作为权重，和全部steps的v做weighted sum。
self attention的输出形状和输入一致的话，则可以叠加多次。
推荐领域也借鉴了self attention，成果便是bst。bst把行为序列和目标物品视为序列，同时为每个step加入时间信息（物品embedding和时间embedding拼接），使用多层self attention。输出作为mlp的输入。

## 改进bst