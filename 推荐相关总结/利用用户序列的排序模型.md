# 背景和主题
在众多高中频应用领域，用户历史行为构成了用户侧主要的信息。
如何充分得利用这些信息对推荐算法和在线推理构成了巨大的挑战。

这个文档主要是介绍和总结个人实践过的利用用户行为序列的排序模型，包括
- base方法(slots+embedding+pooling)
- din 
- dien 
- bst
- 改进bst(在雷鸟的实践)

# 模型演进介绍

## base方法
工业界普遍的排序系统都是slots+embedding方法，也就是多个特征域，
每个域都是离散特征，然后分别经过embedding层后拼接最后多层mlp输出单个或多个得分(比如ctr和cvr分)。  
用户行为序列也不例外,一般是固定行为长度，不足则补齐。然后在embedding层后加入max pooling或者sum pooling。  
这样的方法在线推理方便，不需要改动结构框架。

## din
传统方法+pooling的方法简介得表达了用户的兴趣，但是和目标物品独立。比如为用户推荐饭店和鞋子时，
用户兴趣没有分别强调历史上对同品类的偏好。  
din出于这点考虑，用目标物品和历史物品的相似度分数来作为sum的权重。论文中相似度是通过out product生成的。

#### gru 
gru是简化的lstm，计算方式如下：
$$
u_t = sigmoid(W^u * i_t + U^u * h_{t-1} + B^u)  更新门   
r_t = sigmoid(W^r * i_t + U^r * h_{t-1} + B^r)  重置门  
\hat{h}_t = tanh(W^h + r_t * U^{h} * h_{t-1} + B^h)  新记忆  
h_t = (1 - u_{t}) * h_{t - 1} + u_t * \hat{h}_t  隐状态
$$

## dien
考虑到用户的兴趣会漂移,所以在din在基础上增加了兴趣抽取层Interest Extractor和兴趣演化层Interest Evolution Layer。  
其中抽取层利用了gru,对序列进行了提取，emb_i生成了h_i，并增加了auxiliary loss，也就是用h_i来预测emb_{i+1}被点击和neg sampling物品不被点击的概率。  
兴趣演化层同样用了并改进gru的结构，叫做augru(GRU with Attentional Update gate)，以最后时刻的隐状态作为这个slot的输出。

#### augru  
augru是用注意力系数(weight)改进了更新门，具体来说就是  
用目标物品的emb和h_i生成weight后，weight和更新门点乘作为新的更新门

## bst
17年发表的transformer在nlp届大杀四方，它的核心组件--self attention,就是一个序列输入，每个step生成一组q, k, v向量，借鉴information retrieval领域的术语(query, key, value)，用自己的q乘全部steps的k作为权重，和全部steps的v做weighted sum。
self attention的输出形状和输入一致的话，则可以叠加多次。
推荐领域也借鉴了self attention，成果便是bst。bst把行为序列和目标物品视为序列，同时为每个step加入时间信息（物品embedding和时间embedding拼接），使用多层self attention。输出作为mlp的输入。

## 改进bst