# 背景和主题
在众多高中频应用领域，用户历史行为构成了用户侧主要的信息。
如何充分得利用这些信息对推荐算法和在线推理构成了巨大的挑战。

这个文档主要是介绍和总结个人实践过的利用用户行为序列的排序模型，包括
- base方法(slots+embedding+pooling)
- din 
- dien 
- bst
- 改进bst(在雷鸟的实践)

# 模型演进介绍

## base方法
工业界普遍的排序系统都是slots+embedding方法，也就是多个特征域，
每个域都是离散特征，然后分别经过embedding层后拼接最后多层mlp输出单个或多个得分(比如ctr和cvr分)。  
用户行为序列也不例外,一般是固定行为长度，不足则补齐。然后在embedding层后加入max pooling或者sum pooling。  
这样的方法在线推理方便，不需要改动结构框架。

## din
传统方法+pooling的方法简介得表达了用户的兴趣，但是和目标物品独立。比如为用户推荐饭店和鞋子时，
用户兴趣没有分别强调历史上对同品类的偏好。  
din出于这点考虑，用目标物品和历史物品的相似度分数来作为sum的权重。论文中相似度是通过out product生成的。

#### gru 
gru是简化的lstm，计算方式如下：
$$
u_t = sigmoid(W^u * i_t + U^u * h_{t-1} + B^u)  更新门   
r_t = sigmoid(W^r * i_t + U^r * h_{t-1} + B^r)  重置门  
\hat{h}_t = tanh(W^h + r_t * U^{h} * h_{t-1} + B^h)  新记忆  
h_t = (1 - u_{t}) * h_{t - 1} + u_t * \hat{h}_t  隐状态
$$

## dien
考虑到用户的兴趣会漂移,所以在din在基础上增加了兴趣抽取层Interest Extractor和兴趣演化层Interest Evolution Layer。  
其中抽取层利用了gru,对序列进行了提取，emb_i生成了h_i，并增加了auxiliary loss，也就是用h_i来预测emb_{i+1}被点击和neg sampling物品不被点击的概率。  
兴趣演化层同样用了并改进gru的结构，叫做augru(GRU with Attentional Update gate)，以最后时刻的隐状态作为这个slot的输出。

#### augru  
augru是用注意力系数(weight)改进了更新门，具体来说就是  
用目标物品的emb和h_i生成weight后，weight和更新门点乘作为新的更新门

## bst
17年发表的transformer在nlp届大杀四方，它的核心组件--self attention,就是一个序列输入，每个step生成一组q, k, v向量，借鉴information retrieval领域的术语(query, key, value)，用自己的q乘全部steps的k作为权重，和全部steps的v做weighted sum。然后再叠加point-wise feed-forward network和dropout,layer normalization等防止过拟合的结构。  
self attention的输出形状和输入一致的话，则可以叠加多次。
推荐领域也借鉴了self attention，成果便是bst。bst把行为序列和目标物品视为序列，同时为每个step加入时间信息（物品embedding和时间embedding拼接），使用多层self attention。输出作为mlp的输入。


## utpm  
20年微信的论文,主要突出物料的tag属性和用户的行为序列.  
总体是双塔结构,物料侧用tag id，经过与用户侧共享的tag embedding矩阵构成的embedding层作为物料侧的输出。  
用户侧包含多个fields,比如人口学特征,历史行为序列特征等,每个field最终输出一个维度为1 * E的向量。最重要的field是交互过的物品的tag序列，称为用户tag序列。  
分别经过  
1. embedding层。将每个fields中的离散值映射成1 * E的向量
2. attention层。所有fields共享两个query vector, q1和q2，是为了更好得提取特征重要性。序列field先用q1和q2聚合成一个1 * E的向量，然后所有fields之间再用q1和q2聚合成两个1 * E的向量,这两个向量拼接。
3. cross层，叫做fm-based cross layer，模仿FM来更好学习特征之间交互,把E维(应该是2 * E吧？)每个bit训练一个隐向量，这些向量之间两两相乘，得到的标量再拼接起来，生成E(E+1)/2维的向量，并和本层的输入拼接起来
4. fc(fully-connected)层，就是把cross的输出经过MLP，变成和用户侧一样维度的向量。这里的输出可以作为用户向量。
5. ctr层，fc层输出的用户向量和物料侧的各个标签向量点积后相加，经过sigmoid作为ctr分数，计算最终loss。这里没有单独把点击物料的标签作为正样例的输入，曝光未点击物料的标签作为负样例的输入，是考虑点击一个物料不代表喜欢所有标签。

## 改进bst

