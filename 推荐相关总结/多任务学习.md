# 背景和主题

推荐任务常常有多个目标同时需要优化的需求，比如电商物品展示中提高点击率ctr的同时也要优化转化率cvr，
视频推荐中提高点击率ctr和视频完成率vc。不同任务之间可能强相关，也可能弱相关，样本数量也可能存在巨大差异。


为了解决这个问题，出现了不同的多任务学习模型，包括
- 次要任务目标作为样本权重
- 共享底层权重
- MoE(Mixture of Expert)
- MMoE(Multi-gate MoE)
- ESSM(Entire Space Multi-task Model)

# 模型演进介绍
以两个目标为例

## 次要任务目标作为样本权重
不需要改变单模型结构，只需要调整loss，把次要任务目标作为样本权重。
一般在相关性较高时，主任务效果下降不大时，次要任务效果提升。

## 共享底层权重
模型顶部有两个任务塔，共生成两个loss。  
底层权重共享，比如物品embedding，作为不同任务塔的输入，
适用于相关性比较高的任务。主要是通过联合学习来增大样本量，更好得提取基础特征。

## MOE
多个experts的加权和输入作为不同任务塔的输入。
不同的experts提取不同的特征。
加和的权重是模型输入的函数，称作gate，用来控制各个experts进入的程度。

## MMOE
类似于MOE，学习多个experts,不过不同任务塔的输入是这些experts的不同加权和。
多个门的设计可以更好得捕捉不同任务的差异性。

## ESSM
当任务之间有依赖，比如ctr和cvr，转化是产生在点击之后。
底层embedding共享后,两个MLP塔分别输出ctr和cvr,然后ctr和ctcvr的损失加起来.

