# 背景
随着模型参数和样本数量的增加，单节点上模型保存和训练时长出现挑战。我们这里主要讨论模型训练时间过长的问题。  
主要的解决方案是分布式训练和GPU优化

## 分布式  
分布式训练会把每个更新梯度的节点叫做worker。一般来说，按照worker数量，将参数更新的任务均分到不同的节点，每个节点只负责相应参数的梯度计算。

一般来说，分为两种模式：parameter server模式; all reduce.  
- parameter server模式下，parameter server负责整合梯度和更新模型参数，workers计算梯度.  
- all reduce模式下只有workers,worker之间相互同步整合梯度.

### parameter server模式

parameter server模式下主要分为同步和异步机制.  


| 模式 | 训练迭代| 优点| 缺点|
| -----| -----| ----| ---- | 
| 同步|  所有worker训练同一份batch sample，通过BP来获取各自对应参数的梯度，通信给parameter server；parameter server收集完所有workers的梯度后广播给所有workers再进行新一轮迭代| 梯度更加精准 | 更次迭代被最慢的节点限制| 
| 异步同上| parameter每次收集到1个worker的梯度就更新后广播给所有workers| 可以避免最慢节点的问题| 梯度计算不准确，通信开销更大  |


### AllReduce
参考文献: https://zhuanlan.zhihu.com/p/79030485


AllReduce模式下只有workers,假设
- N个节点
- a为两个通信节点间的latency
- S,Size,allreduce的数据块大小
- C,Computation,每字节数据的计算耗时
- B,Bandwidth表示两个通信节点间的带宽

| 算法 | reduce轮数<img width=20/>| 通信总耗时<img width=100/> | 计算总耗时<img width=100/> | 总耗时<img width=200/> | 瓶颈 |
| :-----:| :----: | :-----: | :-----: | :-----: | :----: |
| Reduce and Broadcast |1 |  a + S/B  |  N * S * C   | 2(a + S/B) + N * S * C   | PS的带宽|
| Recursive halving and doubling | log2N | a + S/B | S * C| 2 * log2N (a + S/B + S*C) | 节点一个时间只send或receive,没有充分利用带宽|
| Butterfly | log2N | a + S/B | S *C | log2N(a + S/B+S*C) | S过大,容易出现延时抖动 |
| Ring AllReduce | N - 1  | a + S/ (N * B) | S * C / N | 2 (N - 1)[a + S / (N * B)] + (N - 1)(S * C/ N)| 需要每个节点上数据足够多 | 
##### Reduce and Broadcast
负责聚合的节点，我们也叫PS,PS收集其他节点的数据后计算再broadcast

##### Recursive Halving and Doubling  
类似于归并排序.  
每轮递归中,每对相邻节点选择一个节点reduce,另一个休息;
直到第log2N轮,到PS完成全部的reduce计算。
![](/images/recursive.webp)


如果N != 2^k
![](/images/recursive2.webp)

# Butterfly 
```python
import math 

def reduce(i, j):
    "节点i向节点j传数据并加和"
    pass 


N = None
n = 1  
K = math.log(N)

# K 轮
for k in range(K):

    for i in range(N):
        reduce(i, i + n if i % (2 * n) < n else i - n) 
    n *= 2
```


![](/images/butterfly.webp)

# RingAllReduce
每个节点上数据分N份,每个节点只和前后两个节点通信.  
总体过程分为scatter reduce和all gather.  
- scatter reduce期间,
```python
def reduce(i, j, k):
    "节点i向节点j传第k部分数据并加和"
    pass 


N = None
for k in range(N - 1):
    for i in range(N):
        reduce(i, (i + 1) % N, (i - k) % N)
```
这样i节点的(i+1) mod N份数据就是全的
- all gather期间，不需要做计算，只需要把数据传给相邻节点
```python
def send(i, j, k):
    "节点i向节点j传第k部分数据"
    pass 


N = None
for k in range(N - 1):
    for i in range(N):
        send(i, (i + 1) % N, (i - k + 1) % N)
```





![](/images/ring_all_reduce.webp)
